function run_modeling_pipeline(cfg):

    # 1) Load raw tables
    train, oil, stores, items, holidays, promos = LOAD_FAVORITA_TABLES(cfg.paths)

    # 2) Preprocess (your functions)
    ext_train = EXTEND_TRAIN(train, stores, items)       # City/State/FAMILY/Class
    oil   = FILL_OIL_MISSING(oil)
    promos = FILL_PROMOS_MISSING(promos)
    holidays = TIDY_HOLIDAYS(holidays)

    # # 3) Join auxiliary tables into train
    # train = JOIN_OIL_PROMOS_HOLIDAYS(ext_train, oil, promos, holidays)

    # 4) Build features for a given dataset slice
    function MAKE_FEATURES(df, max_lag):
        df = DATE_FEATURES(df)
        df = HOLIDAY_FEATURES(df, holidays)
        df = PROMO_FEATURES(df, promos)
        df = LAG_FEATURES(df, lags = [7,14,28,...], max_lag = lags.max())   # uses shift(+)
        df = MOVING_FEATURES(df, windows = [7,14,28])  # shift(1) then rolling
        df = EXPANDING_FEATURES(df, min_periods = 28)  # closed='left'
        return df

    # 3) Join auxiliary tables into train
    train = JOIN_OIL_PROMOS_HOLIDAYS(ext_train, oil, promos, holidays)

    # 5) Time-series cross-validation windows
    frozen_days = COMPUTE_FROZEN_DAYS_FROM_MAX_LAG(max_lag)          # often == max_lag
    windows = BUILD_ROLLING_WINDOWS(
                 full_dates = UNIQUE_DATES(train.date),
                 n_windows = validation_windows,
                 val_size = size_of_validation_windows,
                 stride = cfg.cv.stride_days,
                 embargo = frozen_days,
                 min_train_days = cfg.cv.min_train_days)

    fold_scores = []
    models_per_fold = []

    # 6) Cross-validation loop
    for each (train_range, val_range) in windows:
        tr = SLICE_BY_DATE(train, train_range)
        va = SLICE_BY_DATE(train, val_range)

        Xtr = MAKE_FEATURES(tr, max_lag)
        Xva = MAKE_FEATURES(va, max_lag)

        y_tr = Xtr[cfg.cols.target];  Xtr = DROP_TARGET_COLS(Xtr, cfg.cols.target)
        y_va = Xva[cfg.cols.target];  Xva = DROP_TARGET_COLS(Xva, cfg.cols.target)

        model = TRAIN_MODEL(Xtr, y_tr, Xva, y_va, cfg.model)         # LightGBM / XGBoost + early stop
        preds = PREDICT(model, Xva, cfg.model)

        fold_scores.append( EVALUATE_METRICS(y_va, preds, metrics = ["RMSE","WMAPE"]) )
        models_per_fold.append(model)

    cv_summary = AGGREGATE_FOLD_SCORES(fold_scores)

    HYPERTUNING

    # 7) (Optional) Refit final model on all history up to the end of training
    #    (optionally up to scoring_period.t_min - 1)
    full_hist = SLICE_BY_DATE(train, [MIN_DATE(train), MAX_DATE(train)])
    X_full = MAKE_FEATURES(full_hist, max_lag)
    y_full = X_full[cfg.cols.target];  X_full = DROP_TARGET_COLS(X_full, cfg.cols.target)

    final_model = TRAIN_MODEL_NO_VALIDATION(X_full, y_full, cfg.model)
